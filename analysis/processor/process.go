package processor

import (
	"encoding/json"
	"fmt"
	"log"
	"os"
	"path"
	"sort"

	"github.com/deepsourcelabs/cli/types"
)

const sourceCodeOffset int = 3

type fileContentNode struct {
	Filename    string
	FileContent []string
}

type ProcessAnalysisResults struct {
	CodePath        string
	Processors      []string             // The list of supported post-analysis processors.
	Report          types.AnalysisReport // The report generated by the Analyzer post analysis.
	AnalysisResult  types.AnalysisResult // The final result published post processing the analysis result.
	ProcessedIssues []types.Issue        // List of issues post-processing.
}

/* Accepts the result as a byte array and processes the results in the form of a
 * AnalysisReport struct instance. */
func (p *ProcessAnalysisResults) ProcessAnalysisResult() error {
	// Covert the result from the LSP based format to the default results format.
	p.formatLSPResultsToDefault()

	// Once the result has been converted to the DeepSource format, start processing the issues.
	log.Println("Total issues: ", len(p.AnalysisResult.Issues))
	if len(p.AnalysisResult.Issues) != 0 {
		err := p.processIssues()
		if err != nil {
			return err
		}
	}
	log.Println("Issues after processing: ", len(p.AnalysisResult.Issues))
	b, _ := json.Marshal(p.AnalysisResult.Issues)

	if err := os.WriteFile(path.Join("/Users/phoenix/Code/deepsource/result.json"), b, 0o600); err != nil {
		return err
	}
	return nil
}

/* processIssues sorts the issues in an alphabetical order & processes the issues for the
 * various required processors.
 * As of now, there are two processors supported:
 * - skipcq : Processes the issues and checks if some of them should be ignored since they have
 *            been ignored by the user through suitable `skipcq` comments.
 * - source_code_load :  Processes the issues for the source code snippets, highlights the snippets
 *                       and adds them to the Analysis result. */
func (p *ProcessAnalysisResults) processIssues() error {
	// All the files that appear in the issues are now processed by the processors listed in analyzer conf
	// We must cache the files in order to not do file IO for every processor.
	p.sortIssuesByFile()

	// Get the issues to file range.
	filesIndex := createIssueFileRange(p.AnalysisResult)
	fmt.Println(filesIndex)

	// Generate the silencers regexMap.
	generateSilencersRegexMap()

	// Iterate over the filesIndex and read the files and process the issues using the suitable processors.
	p.processIssuesBatch(filesIndex)
	p.AnalysisResult.Issues = p.ProcessedIssues

	// Sort again for consistency (mostly for test to pass).
	p.sortIssuesByFile()

	return nil
}

/* While this loop looks like it would have a complexity of len(filesWIssueRange) * len(cachedFiles) * issues * len(processorList)
 * it only has a complexity of O(len(report.Issues)).
 * When there are a lot of files to be processed, opening all of them one by one takes time, while the CPU waits idly.
 * Opening all files and loading them into memory is expensive in terms of space, since there could be a lot of files.
 * Hence, opening files concurrently in batches (of, say, 30 files) and then processing all issues in those 30 files one by one
 * appears to be the best option. We cannot process each file's issues concurrently, because only the file loading operation is
 * IO intensive, and the rest is CPU intensive. */
func (p *ProcessAnalysisResults) processIssuesBatch(filesWIssueRange []IssueRange) {
	issueIndex := 0
	batchSize := 30
	maxIssueDensity := 100
	for processedFiles := 0; processedFiles < len(filesWIssueRange); {

		// Process files in batches of `batchSize` to avoid `too many files open` error
		filesToProcess := 0
		if len(filesWIssueRange)-processedFiles < batchSize {
			filesToProcess = len(filesWIssueRange) - processedFiles
		} else {
			filesToProcess = batchSize
		}

		fileContentChannel := make(chan fileContentNode, filesToProcess)
		for j := 0; j < filesToProcess; j++ {
			filename := filesWIssueRange[processedFiles+j].Filename
			go addFileToCache(fileContentChannel, filename, p.CodePath)
		}

		cachedFiles := []fileContentNode{}
		for j := 0; j < filesToProcess; j++ {
			cachedFiles = append(cachedFiles, <-fileContentChannel)
		}

		// sort the cached files by filename, because our issues are sorted by filename
		sort.Slice(cachedFiles, func(i, j int) bool {
			return cachedFiles[i].Filename < cachedFiles[j].Filename
		})

		for j, cachedFile := range cachedFiles {
			// if the number of issues in this file is more than a certain number of issues
			// averaged per line, this may be a generated file. Skip processing of further issues
			// in this file
			linesInThisFile := len(cachedFile.FileContent) | 1 // bitwise op to ensure no divisionbyzero errs
			issuesInThisFile := filesWIssueRange[processedFiles+j].EndIndex - filesWIssueRange[processedFiles+j].BeginIndex
			if (issuesInThisFile / linesInThisFile) > maxIssueDensity {
				log.Printf(
					"Skipping file %s. Too many issues per line. Lines: %d, issues: %d\n",
					cachedFile.Filename,
					linesInThisFile,
					issuesInThisFile,
				)
				p.AnalysisResult.Errors = append(p.AnalysisResult.Errors, types.Error{
					HMessage: fmt.Sprintf(
						"Skipped file %s because too many issues were raised. "+
							"Is this a generated file that can be added in [exclude_patterns](https://deepsource.io/docs/config/deepsource-toml.html#exclude-patterns)?",
						cachedFile.Filename,
					),
					Level: 1,
				})
				continue
			}

			for issueIndex < len(p.AnalysisResult.Issues) {
				issue := p.AnalysisResult.Issues[issueIndex] // initialize the loop

				// Issue is for another file. Go to that file
				if cachedFile.Filename != issue.Location.Path { // loop condition
					break
				}

				processedIssue := issue
				skipIssue := false
				var err error

				// Loop through processors
				for _, processor := range p.Processors {
					switch processor {
					case "source_code_load":
						procLoadSourceCode(cachedFile.FileContent, &processedIssue, sourceCodeOffset)
					case "skip_cq":
						skipIssue, err = procSkipCQ(cachedFile.FileContent, &processedIssue)
						if err != nil {
							log.Println("Publish: Error in processor skip_cq for issue: ", issue, err)
						}
					default:
						log.Println("Publish: Unkown processor received")
					}
				}
				if !skipIssue {
					p.ProcessedIssues = append(p.ProcessedIssues, processedIssue)
				}
				issueIndex++ // loop increments
			}
		}
		log.Println("Batch processing done")
		// Increase total number of files processed
		processedFiles += filesToProcess
	}
}
